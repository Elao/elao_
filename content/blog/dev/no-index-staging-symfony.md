---
type:               "post"
title:              "Comment empÃªcher les moteurs de recherche d'indexer votre app Symfony en staging ?"
date:               "2019-07-10"
lastModified:       ~

description:        "Les pages de votre application n'ont pas vocation Ã  Ãªtre prÃ©sentes dans les moteurs de recherche ? Voici une courte explication pour vous aider Ã  empÃªcher le crawl et l'indexation."

thumbnail:          "content/images/blog/thumbnails/judging-sardine-small.jpg"
banner:             "content/images/blog/headers/judging-sardine-large.jpg"
tags:               ["Symfony", "seo", "no-index"]
categories:         ["Dev", "Symfony", "seo"]
authors:            ["mcolin", "aldeboissieu"]

---
L'indexation par les robots des moteurs de recherche des urls de staging ou de dÃ©monstration sont des cas classiques de [#SEOHorrorStories](https://www.webrankinfo.com/dossiers/conseils/horreurs-du-seo). En effet, cette situation est gÃªnante, pour deux raisons :

- L'entreprise ne souhaite probablement pas exposer Ã  ses concurrents ou aux curieux du travail en cours,
- Le contenu relatif Ã  l'entreprise est disponible sous plusieurs urls, induisant un fort risque de dilution de la pertinence du contenu du site "officiel", puisque celui-ci peut Ãªtre proposÃ© sur deux pages diffÃ©rentes (c'est ce qu'on appelle la duplication de contenu).

Voyons ensemble **quelques solutions pour ne pas indexer les pages publiques de nos applicatifs**, si nous n'en avons pas besoin.

## Les meilleures solutions ğŸ’¡: l'authentification cÃ´tÃ© serveur et le filtre par IP

Le meilleur moyen d'empÃªcher tout crawl des robots et visites des internautes est d'**imposer une authentification cÃ´tÃ© serveur**. Une autre bonne solution est de filtrer l'accÃ¨s au site selon l'IP, c'est Ã  dire qu'on autorise l'affichage que si vous y Ãªtes autorisÃ©s.
Bien sÃ»r, dans les deux cas, les robots ne peuvent pas accÃ©der aux pages, empÃªchant de ce fait tout risque de crawl et donc d'indexation.

NÃ©anmoins, ces deux solutions peuvent parfois entraÃ®ner de nombreuses contraintes, par exemple si vous utilisez diffÃ©rents sous-domaines pour vos assets ou pour vos apis. La recette peut alors se compliquer sur les features les plus importantes du site.


### Le plan B ğŸ‘ : l'en-tÃªte de rÃ©ponse HTTP

Cette **instruction X-Robots-Tag indiquera aux robots de ne pas indexer la page**. Attention, cette mÃ©thode ne doit pas Ãªtre couplÃ©e avec une directive de disallow de l'intÃ©gralitÃ© du robots.txt, puisque les bots n'auraient jamais accÃ¨s Ã  ce tag.

Une des variantes est la balise meta robots noindex, [c'est une des solutions dÃ©crites par Google dans sa documentation officielle](https://support.google.com/webmasters/answer/93710?hl=fr).

### Comment paramÃ©trer ce tag sur Symfony ?

Depuis Symfony 4.3, [une configuration](https://symfony.com/blog/new-in-symfony-4-3-automatic-search-engine-protection) permet d'ajouter automatiquement le header `X-Robots-Tag: noindex` aux rÃ©ponses de Symfony.

```yaml
# config/packages/framework.yaml
framework:
    disallow_search_engine_index: true
```

NÃ©anmoins, **vous ne pouvez modifier cette configuration qu'en fonction de l'environnement Symfony** (`dev`, `prod`, `test`, ...), et non en fonction du serveur. L'idÃ©e serait d'ajouter ce header sur les serveurs de staging, demo ou recette par exemple, et de ne pas l'ajouter sur le serveur de production, peu importe l'environnement Symfony qui est utilisÃ©.

Malheureusement, **cette configuration ne peux pas Ãªtre pilotÃ©e par une variable d'environnement** car elle impacte directement le container (dÃ©finition d'un listener) et Symfony ne permet pas de faire cela au runtime. Je conseille donc de ne pas utiliser cette configuration.

La solution est de passer par la configuration **nginx** ou **Apache** de votre serveur pour ajouter le header. Par exemple avec nginx:

```nginx
server {
    ...
    add_header X-Robots-Tag "noindex";
    ...
}
```

Et avec Apache (sous rÃ©serve que le mod header soit activÃ©) :

```apacheconf
...
SetEnv X-Robots-Tag noindex
...
```

Cette solution a l'avantage de fonctionner **peu importe la version de Symfony, le framework ou le langage utilisÃ© par votre application**. De plus, elle ne pourra pas Ãªtre dÃ©sactivÃ©e lors d'un mauvais dÃ©ploiement.

## La chose Ã  ne pas faire ğŸ™…â€â™€ï¸ : interdire l'indexation via le robots.txt

GrÃ¢ce Ã  la directive `disallow`, on pense pouvoir empÃªcher les robots de visiter et d'indexer les pages de notre site. Mais ce n'est pas tout Ã  fait l'objectif de `disallow`, **qui n'empÃªche pas l'indexation mais le crawl**. RÃ©sultat : vous pouvez retrouver des pages indexÃ©es dans les moteurs de recherche, mÃªme si le robot ne remonte ni meta description, ni title. Vous pouvez ainsi lire le fameux message :

```A description for this result is not available because of this sites's robots.txt```

De plus, il n'est pas rare de trouver en production des fichiers `robots.txt` paramÃ©trÃ©s pour le staging, car ils auraient Ã©tÃ© oubliÃ©s lors de la mise en production ğŸ™€.

Bon Ã  savoir : la directive Noindex, qui Ã©tait rarement utilisÃ©e, [a Ã©tÃ© officiellement abandonnÃ©e par Google](https://webmasters.googleblog.com/2019/07/a-note-on-unsupported-rules-in-robotstxt.html).

## Oups, mon site avec des urls de staging a Ã©tÃ© indexÃ© ...

Faites le nÃ©cessaire pour empÃªcher les futures visites de robots. Vous pouvez ensuite utiliser [l'outil de suppression d'URL de la Search Console](https://www.google.com/webmasters/tools/removals), et demander la suppression des URLs problÃ©matiques.
