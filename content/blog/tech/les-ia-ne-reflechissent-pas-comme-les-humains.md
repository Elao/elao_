---
title: 'Les IA gÃ©nÃ©ratives ne rÃ©flÃ©chissent pas comme les humains'
date: '2024-03-28' # Au format YYYY-MM-DD
lastModified: ~ # Au format YYYY-MM-DD. Pour indiquer explicitement qu'un article a Ã©tÃ© mis Ã  jour
description: "DÃ©couvrez Ã  travers cet article le fonctionnement des IA et plus spÃ©cifiquement des modÃ¨les de langage."
authors: [equentin] # (multiple acceptÃ©s)
tableOfContent: false # `true` pour activer ou `3` pour lister les titres sur 3 niveaux.
tags: [ia, chatgpt, llm, prompt]
thumbnail: content/images/blog/2024/ia/image-cover-ia.jpg
tweetId: '' # Ajouter l'id du Tweet aprÃ¨s publication.
outdated: false # `true` pour marquer un article comme obsolÃ¨te ou une chaÃ®ne de caractÃ¨re pour un message spÃ©cifique Ã  afficher
---

Ã€ la sortie de [**ChatGPT**](https://chat.openai.com/), nous avons Ã©tÃ© nombreuxÂ·euses Ã  tester cet outil qui est apparu comme une nouvelle attraction rÃ©volutionnaire. Nous avons commencÃ© Ã  lui poser des questions, plus ou moins prÃ©cises, Ã  Ã©changer avec lui, Ã  le tester et Ã  tenter de dÃ©couvrir ses limites. 

Au sein de lâ€™agence, nous avons Ã©tÃ© plusieurs Ã  nous intÃ©resser de plus prÃ¨s Ã  ChatGPT mais surtout au fonctionnement de ces IA gÃ©nÃ©ratives afin de mieux les apprÃ©hender. Comprendre le mÃ©canisme de ces outils nous a ainsi permis dâ€™aller plus loin dans leur utilisation et de mesurer lâ€™intÃ©rÃªt quâ€™ils peuvent avoir dans notre quotidien professionnel.ğŸ’¡

Ã€ travers notre veille personnelle, notre participation Ã  des confÃ©rences, Ã  nos Ã©tudes sur des lectures spÃ©cialisÃ©es, nous avons rÃ©alisÃ© une chose essentielle :  **les IA ne possÃ¨dent pas une capacitÃ© de rÃ©flexion similaire Ã  celle des Ãªtres humains . Ce sont avant tout des outils entraÃ®nÃ©s, capables dâ€™assimiler des donnÃ©es et de compiler des informations.** En partant de ce postulat, le fonctionnement de ces assistants et la maniÃ¨re de crÃ©er des prompts nous est apparu plus clair, notamment pour la crÃ©ation de notre produit [**Amabla**](https://www.amabla.com).  

## De maniÃ¨re trÃ¨s simple, comment fonctionne une IA gÃ©nÃ©rative ?

Lâ€™univers de lâ€™intelligence artificielle est prÃ©sent depuis dÃ©jÃ  de nombreuses annÃ©es (dÃ¨s les annÃ©es 50). Avec le dÃ©veloppement de lâ€™informatique et son dÃ©ploiement auprÃ¨s du grand public, les IA ont commencÃ© Ã  connaÃ®tre un succÃ¨s important dans les annÃ©es 2000, notamment avec lâ€™une des avancÃ©es majeures de lâ€™IA : le **machine learning** (ou lâ€™art pour les machines dâ€™apprendre par elles-mÃªmes en analysant des donnÃ©es et en amÃ©liorant leurs performances au fil du temps). 

Câ€™est dans les annÃ©es 2020 que nous allons voir apparaÃ®tre la notion de **modÃ¨le de fondation** (capable de comprendre et de gÃ©nÃ©rer du contenu) connaissant une popularisation grandissante avec lâ€™arrivÃ©e de lâ€™outil ChatGPT.

## Comment rÃ©sumer simplement le fonctionnement dâ€™une IA gÃ©nÃ©rative basÃ©e sur les modÃ¨les de langage (ou LLM) ? 

Le principe des **LLM** (Large Language Models) repose sur lâ€™utilisation de **rÃ©seaux de neurones** Ã  grande Ã©chelle pour analyser, comprendre et gÃ©nÃ©rer du texte. Ces systÃ¨mes sont entraÃ®nÃ©s sur des grands corpus de texte leur permettant de comprendre des modÃ¨les linguistiques, des structures grammaticales, des associations de mots et des nuances de langage. 

Les LLM sont Ã©galement dotÃ©s dâ€™une certaine spÃ©cificitÃ© : la **prÃ©diction**. En effet, les LLM fonctionnent en prÃ©disant le mot suivant dans une sÃ©quence de mots, en se basant sur les mots prÃ©cÃ©dents. Cela leur permet de concevoir des contenus cohÃ©rents et contextuellement pertinents. 

Pour comprendre un peu plus prÃ©cisÃ©ment le fonctionnement des IA gÃ©nÃ©ratives sans entrer dans des dÃ©tails trop techniques, il est important de se focaliser sur plusieurs points dâ€™attentionğŸ‘‡

## Comprendre leur fonctionnement

### Les IA gÃ©nÃ©ratives ne nous rÃ©pondent pas en langage naturel 

Lorsque nous formulons notre prompt, lâ€™IA ne va pas prendre en considÃ©ration les mots en tant que tel, mais va, via le processus de tokenisation, convertir le texte en une sÃ©rie de vecteurs numÃ©riques. Le texte va alors Ãªtre transformÃ© en **token**. Le principe des tokens est primordial pour les modÃ¨les dâ€™IA basÃ©s sur le langage car ils permettent de convertir le texte en **donnÃ©es structurÃ©es que les algorithmes peuvent traiter**. 

Plus concrÃ¨tement, les LLM ne vont pas travailler directement sur les mots en eux-mÃªmes, mais vont les dÃ©composer. GÃ©nÃ©ralement, un token correspond Ã  Â¾ dâ€™un mot. 

<figure>
    <img src="content/images/blog/2024/ia/nb-tokens-openai.png" alt="Calculer le nombre de token sur une phrase" width="650">
    <figcaption>
      <span class="figure__legend">Calculer le nombre de token sur une phrase</span>
    </figcaption>
</figure>

!!! note "Pour info"
    Si vous souhaitez comprendre le fonctionnement de tokÃ©nisation et tester vos propres contenus, OpenAI propose un outil qui comptabilise votre phrase en tokens (en fonction du modÃ¨le - ChatGPT 3 OU 4) https://platform.openai.com/tokenizer

Chaque LLM possÃ¨de son propre fonctionnement de tokenisation et est capable de gÃ©nÃ©rer plus ou moins de tokens. Par exemple, actuellement **ChatGPT 4** (OpenAI) peut gÃ©nÃ©rer en une seule fois (ce quâ€™on appelle la fenÃªtre de contexte) 8 000 tokens, soit 6 000 mots. Ã€ lâ€™inverse, **Gemini** (Google), peut gÃ©nÃ©rer 32 000 tokens, soit 24 000 mots. 

### Les IA gÃ©nÃ©ratives adaptent les mots Ã  leur propre langage 

Pour rappel, les modÃ¨les dâ€™IA gÃ©nÃ©ratives, se reposant sur des principes mathÃ©matiques, ne sont pas capables de comprendre un mot en langage naturel. Elles vont donc devoir trouver un moyen de le transformer dans leur langage Ã  elles. Pour cela, elles vont transformer un mot en un vecteur. 

Avec ce principe lÃ , elles vont pouvoir traiter le terme et donner une suite logique Ã  ce dernier. Bien quâ€™un autre Ã©lÃ©ment va intervenir afin de produire un contenu encore plus prÃ©cis. 

### Les IA gÃ©nÃ©ratives analysent le sens des mots

Afin de pouvoir anticiper le prochain mot et crÃ©er un contenu cohÃ©rent, les IA vont devoir positionner un terme dans son ensemble dans le but de lui donner du sens, via un systÃ¨me de coordonnÃ©es. Câ€™est ce que lâ€™on appelle lâ€™**embedding**. 

Prenons lâ€™exemple dÃ©veloppÃ© par **Science Ã‰tonnante**. Nous souhaitons positionner le mot â€œchatâ€. Nous prenons deux dimensions en compte : sa domestication et sa taille. 
Ainsi, nous positionnons le chat (bien domestiquÃ© et plutÃ´t petit) sur un graphique. Le chien va apparaÃ®tre Ã  cÃ´tÃ© (car encore plus domestiquÃ© et un peu plus grand). Si nous souhaitons positionner le lynx, ce dernier va apparaÃ®tre au-dessus du chat (car plus sauvage et plus grand), de mÃªme pour le loup. Ainsi, on va attribuer Ã  chacun de ces termes deux nombres, comprenant donc deux dimensions. Cela va nous permettre de positionner un terme dans son ensemble.â›“ï¸ 

<figure>
    <img src="content/images/blog/2024/ia/embedding.png" alt="exemple du systÃ¨me d'embedding" width="650">
    <figcaption>
      <span class="figure__legend">Exemple du systÃ¨me d'embedding</span>
    </figcaption>
</figure>

Dans cet exemple, nous prenons en compte 2 nombres pour 2 dimensions, mais il faut bien comprendre que pour des textes plus consÃ©quents, cela ne sera pas suffisant. Il faudra alors plus de dimension, gÃ©nÃ©ralement 300, afin de placer le mot dans â€œlâ€™espace des significationsâ€.

Avec ce principe, les IA gÃ©nÃ©ratives sont capables de prÃ©voir la suite des mots de maniÃ¨re plus probable et donc de concevoir une rÃ©ponse plus poussÃ©e, plus cohÃ©rente. 

## L'essentiel Ã  retenir 

Comme lâ€™indique Flavien Chervet dans son ouvrage *Hyperprompt - MaÃ®triser lâ€™art du prompt engineering*, il faut â€œ*comprendre que les LLM ne rÃ©pondent pas aux demandes de lâ€™utilisateur en â€œrÃ©flÃ©chissant*â€ Ã  la rÃ©ponse pertinente. Ils prennent la demande de lâ€™utilisateur comme une nouvelle donnÃ©e, et tentent de **prÃ©dire** les prochains mots les plus **probables** par rapport Ã  cette demande.â€

Autrement dit, le modÃ¨le ne cherche pas Ã  dire quelque chose de vrai, il nâ€™a pas cette dimension de vÃ©ritÃ©. Il cherche Ã  proposer quelque chose de **plausible** par rapport Ã  ce quâ€™il a pu â€œapprendreâ€ durant ses phases dâ€™entraÃ®nement. 

Il est important de saisir ce fonctionnement afin de pouvoir rÃ©aliser des prompts percutants qui vous permettront de maximiser lâ€™apport des IA gÃ©nÃ©ratives dans votre contexte mÃ©tier. MaÃ®triser lâ€™art du **prompting** vous permettra dâ€™Ãªtre plus efficace dans vos tÃ¢ches quotidiennes. 

Chez Elao, nous avons mesurÃ© lâ€™impact que les IA gÃ©nÃ©ratives pouvaient avoir dans nos mÃ©tiers et nous avons souhaitÃ© travailler sur une solution adaptÃ©e au contexte de chacun : regrouper dans un seul et mÃªme outil tous les assistants dont vous avez besoin. Vous pouvez ainsi **crÃ©er vos assistants sur-mesure ou utiliser des modÃ¨les prÃ©-Ã©tablis**, afin de vous accompagner dans **vos tÃ¢ches hebdomadaires** et vous dÃ©gager du temps sur les sujets pour lesquels vous avez une vÃ©ritable valeur ajoutÃ©e. Mais pour cela, il nous semblait essentiel de vous partager le fonctionnement des IA, afin que vous puissiez profiter pleinement de cette solution. 

Si vous souhaitez en savoir plus sur Amabla, rendez-vous sur **https://www.amabla.com** ou [contactez nous](https://www.elao.com/contact). 

# Sources

- Hyperprompt - MaÃ®triser l'art du prompt engineering de Flavien CHERVET
- [Ce qui se cache derriÃ¨re ChatGPT](https://www.youtube.com/watch?v=CsQNF9s78Nc ) - Science Ã‰tonnante 
- [Comment les IA font-elles pour comprendre notre langue ?](https://www.youtube.com/watch?v=7ell8KEbhJo ) - Science Ã‰tonnante

